{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Inference",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaavEKP8plCq"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaQMtAXDhfxi"
      },
      "source": [
        "#class for splitting up SMILE strings into tokens and one hot encoding them\n",
        "class SMILES_Tokenizer(object):\n",
        "  def __init__(self):\n",
        "      \n",
        "      #creating list of all characters in SMILE encoding\n",
        "      atoms = [\n",
        "            'Li', 'Na', 'Al', 'Si', 'Cl', 'Sc', 'Zn', 'As', 'Se', 'Br', 'Sn',\n",
        "            'Te', 'Cn', 'H', 'B', 'C', 'N', 'O', 'F', 'P', 'S', 'K', 'V', 'I'\n",
        "      ]\n",
        "\n",
        "      special = [\n",
        "            '(', ')', '[', ']', '=', '#', '%','.', '0', '1', '2', '3', '4', '5',\n",
        "            '6', '7', '8', '9', '+', '-', 'se', 'te', 'c', 'n', 'o', 's'\n",
        "      ]\n",
        "\n",
        "      #creating list of characters used for starting string, padding string, \n",
        "      #and ending string\n",
        "      padding = ['G', 'A', 'E']\n",
        "\n",
        "      #creating total list of characters\n",
        "      self.characters = sorted(atoms, key=len, reverse=True) + special + padding\n",
        "      dict_len = len(self.characters)\n",
        "\n",
        "      self.dict_len1 = [x for x in self.characters if len(x) == 1]\n",
        "      self.dict_len2 = [x for x in self.characters if len(x) == 2]\n",
        "\n",
        "      self.one_hot_dict = {}\n",
        "      \n",
        "      # creating one hot encoding vector for each character\n",
        "      for i, char in enumerate(self.characters):\n",
        "        vec = np.zeros(dict_len, dtype=np.float32)\n",
        "        vec[i] = 1\n",
        "        self.one_hot_dict[char] = vec\n",
        "      self.one_hot_dict[-1] = np.full(dict_len, dtype=np.float32, fill_value=-1)\n",
        "\n",
        "\n",
        "\n",
        "  #splitting SMILE string into tokens\n",
        "  def tokenize(self, smiles):\n",
        "    char_Count = len(smiles)\n",
        "    smiles += ''\n",
        "    tokens = []\n",
        "    i = 0\n",
        "    z = 0\n",
        "    while(i<char_Count):\n",
        "      \n",
        "      #checking if next character has length 2 if so, finding what character it\n",
        "      #is then appending it to list of tokens\n",
        "      if smiles[i:i+2] in self.dict_len2:\n",
        "        tokens.append(smiles[i:i+2])\n",
        "        i+=2  \n",
        "        continue\n",
        "      \n",
        "      #checking if character has length 1 if so, finding waht character it is\n",
        "      #then appending it to list of tokens\n",
        "      if smiles[i:i+1] in self.dict_len1:\n",
        "        tokens.append(smiles[i:i+1])\n",
        "        i+=1\n",
        "        continue\n",
        "\n",
        "      z +=1\n",
        "    return tokens\n",
        "  \n",
        "  def one_hot_encode(self, tokenized_smiles):\n",
        "    encoded_smiles = np.array([self.one_hot_dict[symbol] for symbol in tokenized_smiles],np.float32)\n",
        "    encoded_smiles = encoded_smiles.reshape(encoded_smiles.shape[0], encoded_smiles.shape[1])\n",
        "    tensor = tf.convert_to_tensor(encoded_smiles, dtype=tf.float32)\n",
        "    return encoded_smiles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHQmXDA6dtgK"
      },
      "source": [
        "class Data_Processing(object):\n",
        "  def __init__(self, max_len):\n",
        "    self.tokenizer = SMILES_Tokenizer()\n",
        "    self.one_hot_dict = self.tokenizer.one_hot_dict\n",
        "    self.dictionary = self.tokenizer.characters\n",
        "\n",
        "    #setting max length for SMILE strings\n",
        "    self.max_len = max_len\n",
        "\n",
        "  #split SMILE string into tokens\n",
        "  def tokenize(self, smi):\n",
        "    return self.tokenizer.tokenize(smi)\n",
        "\n",
        "  #tokenize for batches\n",
        "  def tokenize_data(self, data):\n",
        "    tokenized_smiles = [self.tokenizer.tokenize(smi) for smi in data]\n",
        "    return tokenized_smiles  \n",
        "      \n",
        "  #one hot encode tokenized SMILE string\n",
        "  def one_hot_encode(self, pad_smi):\n",
        "    return self.tokenizer.one_hot_encode(pad_smi)\n",
        "  \n",
        "  #one_hot_encode for batches\n",
        "  def one_hot_encoding(self, data):\n",
        "    encoded_smiles = [self.tokenizer.one_hot_encode(pad_smi) for pad_smi in data]\n",
        "    return encoded_smiles\n",
        "\n",
        "  #adding padding and adding Start and End tokens to SMILE strings\n",
        "  def pad(self, tokenized_smi):\n",
        "    return ['G'] + tokenized_smi + ['E'] + [\n",
        "      'A' for _ in range(self.max_len - len(tokenized_smi))\n",
        "    ]\n",
        "\n",
        "  #padding for batches\n",
        "  def padding(self, data):\n",
        "    padded_smiles = [self.pad(t_smi) for t_smi in data]\n",
        "    return padded_smiles\n",
        "\n",
        "  def get_dictionary(self):\n",
        "    return self.one_hot_dict\n",
        "\n",
        "  #mapping one-hot encoded array to character\n",
        "  def basic_inverse_dictionary(self, encoded_str):\n",
        "    \n",
        "    decoded_str = [np.where(vector == 1) for vector in encoded_str]\n",
        "    return [self.characters[index] for index in decoded_str[0][1]]\n",
        "\n",
        "  #mapping index to character\n",
        "  def inverse_dictionary(self, index):\n",
        "    return self.characters[index]\n",
        "\n",
        "  #inverse_dictionary for batches\n",
        "  def inverse_dictionary_(self, index):\n",
        "    return [self.characters[i] for i in index]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOtHHlGGi8nT"
      },
      "source": [
        "#network archtecture definition\n",
        "class GRU(tf.keras.Model):\n",
        "  def __init__(self, time_steps, vocab_size, batch_size):\n",
        "    super(GRU ,self).__init__()\n",
        "    #first hidden layer\n",
        "    self.gru1 = tf.keras.layers.GRU(\n",
        "                                   enc_units,\n",
        "                                   return_sequences = True,\n",
        "                                   return_state = True,\n",
        "                                   recurrent_initializer='glorot_uniform',\n",
        "                                   dropout = 0.3)\n",
        "    #second hidden layer\n",
        "    self.gru2 = tf.keras.layers.GRU(\n",
        "                                   enc_units,\n",
        "                                   return_sequences = True,\n",
        "                                   return_state = True,\n",
        "                                   recurrent_initializer='glorot_uniform',\n",
        "                                   dropout = 0.3)\n",
        "    #third hidden layer\n",
        "    self.gru3 = tf.keras.layers.GRU(\n",
        "                                   enc_units,\n",
        "                                   return_sequences = True,\n",
        "                                   return_state = True,\n",
        "                                   recurrent_initializer='glorot_uniform',\n",
        "                                   dropout = 0.3)      \n",
        "    self.vocab_size = vocab_size\n",
        "    self.time_steps = time_steps\n",
        "    self.batch_size = batch_size\n",
        "    \n",
        "    #dense layer doesnt have softmax, softmax is only used for inference as its\n",
        "    #supposed to make model training more stable\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size,\n",
        "                                       kernel_initializer='glorot_uniform')\n",
        "\n",
        "  #forward step for calculating prediction for next time step and hidden states\n",
        "  def call(self, x, hidden_state1, hidden_state2, hidden_state3):\n",
        "    \n",
        "    #calculating hidden states and passing them along to next hidden layer\n",
        "    output, hidden_state1 = self.gru1(x, hidden_state1)\n",
        "    output, hidden_state2 = self.gru2(output, hidden_state2)\n",
        "    output, hidden_state3 = self.gru3(output, hidden_state3)\n",
        "    \n",
        "    #setting tensor dimensions for compatibility with dense output layer\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "    output = self.dense(output)\n",
        "    \n",
        "    return output, hidden_state1, hidden_state2, hidden_state3\n",
        "\n",
        "  #function for setting initial hidden states\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_size, enc_units))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnX47deLi6se"
      },
      "source": [
        "#setting parameters\n",
        "BATCH_SIZE = 1024\n",
        "enc_units = 1024\n",
        "vocab_inp_size = 53\n",
        "vocab_tar_size = 53\n",
        "learning_rate = 0.0001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XggipU87pyeC"
      },
      "source": [
        "#creating loss calculator and optimizer objects\n",
        "loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS-1xRu-diVc"
      },
      "source": [
        "#creating reccurent neural network\n",
        "gru = GRU(62, 53, BATCH_SIZE)\n",
        "\n",
        "#loading model weights to sample from\n",
        "checkpoint_dir = \"/content/drive/MyDrive/GRU_CHEM_V15/ckpt-27\"\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer, GRU=gru)\n",
        "options = tf.train.CheckpointOptions(experimental_io_device=\"/job:localhost\")\n",
        "checkpoint.restore(checkpoint_dir, options=options)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLzC62AIdoml"
      },
      "source": [
        "#generates molecules in batches\n",
        "def generate(sample_temp, batches):\n",
        "  data = Data_Processing(60)\n",
        "  sequence = []\n",
        "  start = time.time()\n",
        "  for j in range(batches):\n",
        "    temp = []\n",
        "    \n",
        "    #setting initial states for model\n",
        "    prediction = np.repeat(data.one_hot_encode('G'), 1024, axis=0)\n",
        "    GRU_hidden1 = GRU_hidden2 = GRU_hidden3 = tf.zeros((1024, enc_units))\n",
        "    \n",
        "    for i in range(75):\n",
        "      #adding dimension to input so it is correct dimension for GRU \n",
        "      GRU_input = tf.expand_dims(prediction, 1)\n",
        "      \n",
        "      #calculating new prediction and new hidden states\n",
        "      prediction, GRU_hidden1, GRU_hidden2, GRU_hidden3 = gru(GRU_input, GRU_hidden1, GRU_hidden2, GRU_hidden3)\n",
        "      \n",
        "      #choosing class based on model prediction\n",
        "      idx = sample_with_temp(prediction, sample_temp)\n",
        "      temp += [data.inverse_dictionary_(idx)]\n",
        "      \n",
        "      #setting new prediction so it can be fed back into model\n",
        "      prediction = data.one_hot_encode(data.inverse_dictionary_(idx))\n",
        "\n",
        "    #reorienting array from x by y to y by x \n",
        "    sequence.append(np.array(temp).transpose())\n",
        "  \n",
        "  #collapsing 2d list to 1d\n",
        "  sequence = list(itertools.chain.from_iterable(sequence))\n",
        "  print(time.time() - start)\n",
        "  \n",
        "  #removing padding characters from SMILE string\n",
        "  for i, smi in enumerate(sequence):\n",
        "    sequence[i] = ''.join(smi)\n",
        "    sequence[i] = sequence[i].strip('E')\n",
        "    sequence[i] = sequence[i].strip('A')\n",
        "    sequence[i] = sequence[i].strip('E')\n",
        "  \n",
        "  return sequence\n",
        "\n",
        "#given logits for different classes and a sampling temperature, return a class\n",
        "def sample_with_temp(prediction, sampling_temp):\n",
        "  #calculating probabilities using softmax\n",
        "  prediction = tf.nn.softmax(logits=prediction)\n",
        "  \n",
        "  #normalizing probabilities by sampling_temp parameter\n",
        "  stretched = np.log(prediction) / sampling_temp\n",
        "\n",
        "  #recalculating probabilities after normalization\n",
        "  stretched_probs = np.exp(stretched) / np.sum(np.exp(stretched), axis=1)[:,None]\n",
        "  \n",
        "  #choosing class bsed on probability\n",
        "  return [np.random.choice(range(len(stretched[0])), p=stretched_prob) for stretched_prob in stretched_probs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G89OhroGjnyN"
      },
      "source": [
        "generated_testing = generate(0.5, 1)\n",
        "generated_testing = generated_testing[:500]\n",
        "\n",
        "with open('generated_testing', 'wb') as fp:\n",
        "    pickle.dump(generated_testing, fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNiBSqPOjEQ2"
      },
      "source": [
        "generated = generate(0.75, 98)\n",
        "generated_malaria = generated[:10**5]\n",
        "\n",
        "with open('generated_malaria', 'wb') as fp:\n",
        "    pickle.dump(generated_malaria, fp)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}