{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generator GRU",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzBGtowKCTLW"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "import pickle\n",
        "import moses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p29fuk9J3aNC"
      },
      "source": [
        "train_data = moses.get_dataset('train')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xte-bRof9bBu"
      },
      "source": [
        "#class for splitting up SMILE strings into tokens and one hot encoding them\n",
        "class SMILES_Tokenizer(object):\n",
        "  def __init__(self):\n",
        "      \n",
        "      #creating list of all characters in SMILE encoding\n",
        "      atoms = [\n",
        "            'Li', 'Na', 'Al', 'Si', 'Cl', 'Sc', 'Zn', 'As', 'Se', 'Br', 'Sn',\n",
        "            'Te', 'Cn', 'H', 'B', 'C', 'N', 'O', 'F', 'P', 'S', 'K', 'V', 'I'\n",
        "      ]\n",
        "\n",
        "      special = [\n",
        "            '(', ')', '[', ']', '=', '#', '%','.', '0', '1', '2', '3', '4', '5',\n",
        "            '6', '7', '8', '9', '+', '-', 'se', 'te', 'c', 'n', 'o', 's'\n",
        "      ]\n",
        "\n",
        "      #creating list of characters used for starting string, padding string, \n",
        "      #and ending string\n",
        "      padding = ['G', 'A', 'E']\n",
        "\n",
        "      #creating total list of characters\n",
        "      self.characters = sorted(atoms, key=len, reverse=True) + special + padding\n",
        "      dict_len = len(self.characters)\n",
        "\n",
        "      self.dict_len1 = [x for x in self.characters if len(x) == 1]\n",
        "      self.dict_len2 = [x for x in self.characters if len(x) == 2]\n",
        "\n",
        "      self.one_hot_dict = {}\n",
        "      \n",
        "      # creating one hot encoding vector for each character\n",
        "      for i, char in enumerate(self.characters):\n",
        "        vec = np.zeros(dict_len, dtype=np.float32)\n",
        "        vec[i] = 1\n",
        "        self.one_hot_dict[char] = vec\n",
        "      self.one_hot_dict[-1] = np.full(dict_len, dtype=np.float32, fill_value=-1)\n",
        "\n",
        "\n",
        "\n",
        "  #splitting SMILE string into tokens\n",
        "  def tokenize(self, smiles):\n",
        "    char_Count = len(smiles)\n",
        "    smiles += ''\n",
        "    tokens = []\n",
        "    i = 0\n",
        "    z = 0\n",
        "    while(i<char_Count):\n",
        "      \n",
        "      #checking if next character has length 2 if so, finding what character it\n",
        "      #is then appending it to list of tokens\n",
        "      if smiles[i:i+2] in self.dict_len2:\n",
        "        tokens.append(smiles[i:i+2])\n",
        "        i+=2  \n",
        "        continue\n",
        "      \n",
        "      #checking if character has length 1 if so, finding waht character it is\n",
        "      #then appending it to list of tokens\n",
        "      if smiles[i:i+1] in self.dict_len1:\n",
        "        tokens.append(smiles[i:i+1])\n",
        "        i+=1\n",
        "        continue\n",
        "\n",
        "      z +=1\n",
        "    return tokens\n",
        "  \n",
        "  def one_hot_encode(self, tokenized_smiles):\n",
        "    encoded_smiles = np.array([self.one_hot_dict[symbol] for symbol in tokenized_smiles],np.float32)\n",
        "    encoded_smiles = encoded_smiles.reshape(encoded_smiles.shape[0], encoded_smiles.shape[1])\n",
        "    tensor = tf.convert_to_tensor(encoded_smiles, dtype=tf.float32)\n",
        "    return encoded_smiles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUbEvb10Sc1T"
      },
      "source": [
        "class Data_Processing(object):\n",
        "  def __init__(self, max_len):\n",
        "    self.tokenizer = SMILES_Tokenizer()\n",
        "    self.one_hot_dict = self.tokenizer.one_hot_dict\n",
        "    self.dictionary = self.tokenizer.characters\n",
        "\n",
        "    #setting max length for SMILE strings\n",
        "    self.max_len = max_len\n",
        "\n",
        "  #split SMILE string into tokens\n",
        "  def tokenize(self, smi):\n",
        "    return self.tokenizer.tokenize(smi)\n",
        "\n",
        "  #tokenize for batches\n",
        "  def tokenize_data(self, data):\n",
        "    tokenized_smiles = [self.tokenizer.tokenize(smi) for smi in data]\n",
        "    return tokenized_smiles  \n",
        "      \n",
        "  #one hot encode tokenized SMILE string\n",
        "  def one_hot_encode(self, pad_smi):\n",
        "    return self.tokenizer.one_hot_encode(pad_smi)\n",
        "  \n",
        "  #one_hot_encode for batches\n",
        "  def one_hot_encoding(self, data):\n",
        "    encoded_smiles = [self.tokenizer.one_hot_encode(pad_smi) for pad_smi in data]\n",
        "    return encoded_smiles\n",
        "\n",
        "  #adding padding and adding Start and End tokens to SMILE strings\n",
        "  def pad(self, tokenized_smi):\n",
        "    return ['G'] + tokenized_smi + ['E'] + [\n",
        "      'A' for _ in range(self.max_len - len(tokenized_smi))\n",
        "    ]\n",
        "\n",
        "  #padding for batches\n",
        "  def padding(self, data):\n",
        "    padded_smiles = [self.pad(t_smi) for t_smi in data]\n",
        "    return padded_smiles\n",
        "\n",
        "  def get_dictionary(self):\n",
        "    return self.one_hot_dict\n",
        "\n",
        "  #mapping one-hot encoded array to character\n",
        "  def basic_inverse_dictionary(self, encoded_str):\n",
        "    \n",
        "    decoded_str = [np.where(vector == 1) for vector in encoded_str]\n",
        "    return [self.characters[index] for index in decoded_str[0][1]]\n",
        "\n",
        "  #mapping index to character\n",
        "  def inverse_dictionary(self, index):\n",
        "    return self.characters[index]\n",
        "\n",
        "  #inverse_dictionary for batches\n",
        "  def inverse_dictionary_(self, index):\n",
        "    return [self.characters[i] for i in index]\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osnEpFIPhbHj"
      },
      "source": [
        "#creating generator function to proccess data in batches\n",
        "def load_data(data, batch_size):\n",
        "  tokenizer = Data_Processing(60)\n",
        "  tokens = []\n",
        "  padded_tokens = []\n",
        "  encoded_tokens = []\n",
        "  \n",
        "  #calculating number of batchesin total data\n",
        "  steps = len(data) // batch_size\n",
        "\n",
        "  \n",
        "  i = 0\n",
        "  \n",
        "  while True:\n",
        "    #applying all proccessing to data\n",
        "    tokens = tokenizer.tokenize_data(data[i*batch_size:(i+1)*batch_size])\n",
        "    padded_tokens = tokenizer.padding(tokens)\n",
        "    encoded_tokens = np.array(tokenizer.one_hot_encoding(padded_tokens))\n",
        "    i+=1\n",
        "    if(i == steps):\n",
        "      i = 0\n",
        "      \n",
        "    \n",
        "    yield encoded_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxd0wYTLSf_4"
      },
      "source": [
        "#setting hyperparameters for model\n",
        "BATCH_SIZE = 1024\n",
        "enc_units = 1024\n",
        "vocab_inp_size = 53\n",
        "vocab_tar_size = 53\n",
        "learning_rate = 0.0001\n",
        "batch_steps = len(train_data) // BATCH_SIZE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeawXXP6kbj-"
      },
      "source": [
        "#creating loss calculator and optimizer objects\n",
        "loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPWF5yFrK7B-"
      },
      "source": [
        "#network archtecture definition\n",
        "class GRU(tf.keras.Model):\n",
        "  def __init__(self, time_steps, vocab_size, batch_size):\n",
        "    super(GRU ,self).__init__()\n",
        "    #first hidden layer\n",
        "    self.gru1 = tf.keras.layers.GRU(\n",
        "                                   enc_units,\n",
        "                                   return_sequences = True,\n",
        "                                   return_state = True,\n",
        "                                   recurrent_initializer='glorot_uniform',\n",
        "                                   dropout = 0.3)\n",
        "    #second hidden layer\n",
        "    self.gru2 = tf.keras.layers.GRU(\n",
        "                                   enc_units,\n",
        "                                   return_sequences = True,\n",
        "                                   return_state = True,\n",
        "                                   recurrent_initializer='glorot_uniform',\n",
        "                                   dropout = 0.3)\n",
        "    #third hidden layer\n",
        "    self.gru3 = tf.keras.layers.GRU(\n",
        "                                   enc_units,\n",
        "                                   return_sequences = True,\n",
        "                                   return_state = True,\n",
        "                                   recurrent_initializer='glorot_uniform',\n",
        "                                   dropout = 0.3)      \n",
        "    self.vocab_size = vocab_size\n",
        "    self.time_steps = time_steps\n",
        "    self.batch_size = batch_size\n",
        "    \n",
        "    #dense layer doesnt have softmax, softmax is only used for inference as its\n",
        "    #supposed to make model training more stable\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size,\n",
        "                                       kernel_initializer='glorot_uniform')\n",
        "\n",
        "  #forward step for calculating prediction for next time step and hidden states\n",
        "  def call(self, x, hidden_state1, hidden_state2, hidden_state3):\n",
        "    \n",
        "    #calculating hidden states and passing them along to next hidden layer\n",
        "    output, hidden_state1 = self.gru1(x, hidden_state1)\n",
        "    output, hidden_state2 = self.gru2(output, hidden_state2)\n",
        "    output, hidden_state3 = self.gru3(output, hidden_state3)\n",
        "    \n",
        "    #setting tensor dimensions for compatibility with dense output layer\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "    output = self.dense(output)\n",
        "    \n",
        "    return output, hidden_state1, hidden_state2, hidden_state3\n",
        "\n",
        "  #function for setting initial hidden states\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_size, enc_units))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "M3rnnyMPcqOI",
        "outputId": "b1e619d8-1bb9-4fc6-ec48-f0a3e1c8b6e2"
      },
      "source": [
        "import time\n",
        "import os\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "#initializing TPU\n",
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
        "\n",
        "strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
        "\n",
        "with strategy.scope():\n",
        "  \n",
        "  #initializing neural network\n",
        "  gru = GRU(62, 53, BATCH_SIZE)\n",
        "\n",
        "  #choosing checkpoint to save weights too\n",
        "  checkpoint_dir = '/content/drive/MyDrive/GRU_CHEM_V15'\n",
        "  checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "  options = tf.train.CheckpointOptions(experimental_io_device=\"/job:localhost\")\n",
        "  checkpoint = tf.train.Checkpoint(optimizer=optimizer, GRU=gru)\n",
        "  \n",
        "  #Weights can be restored like this:\n",
        "  #checkpoint.restore('/content/drive/MyDrive/GRU_CHEM_V13/ckpt-24', options=options)\n",
        "  \n",
        "  \n",
        "  @tf.function\n",
        "  def train_step(inp, targ, GRU_hidden1, GRU_hidden2, GRU_hidden3, teacher_forcing):\n",
        "    loss = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "      for t in range(1, targ.shape[1] - 1):\n",
        "        \n",
        "        #if teacher forcing is true or this is first input to neural net,\n",
        "        #pass the the target from previous time step to model\n",
        "        if t == 1 or random.random() < teacher_forcing:\n",
        "          GRU_input = tf.expand_dims(targ[:,t], 1)\n",
        "        \n",
        "        #if teacher forcing is false, pass previous prediction as next input\n",
        "        #to neural net\n",
        "        else:\n",
        "          GRU_input = tf.expand_dims(prediction, 1)\n",
        "          GRU_input = tf.one_hot(tf.argmax(GRU_input, 2), 53)\n",
        "        \n",
        "        #calculating next hidden states and predictions of model\n",
        "        prediction, GRU_hidden1, GRU_hidden2, GRU_hidden3 = gru(GRU_input, GRU_hidden1, GRU_hidden2, GRU_hidden3)\n",
        "        \n",
        "        #calculating loss \n",
        "        loss += tf.nn.compute_average_loss(loss_object(targ[:,t+1], prediction), global_batch_size=BATCH_SIZE)\n",
        "      \n",
        "      \n",
        "    #calculating loss for batch\n",
        "    batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "    variables = gru.trainable_variables\n",
        "\n",
        "    #calculating gradients\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    #using gradients to update network weights\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return batch_loss\n",
        "  \n",
        "  #wrapping train step so it can run on TPU\n",
        "  @tf.function\n",
        "  def distributed_train_step(inp, targ, GRU_hidden1, GRU_hidden2, GRU_hidden3, teacher_forcing):\n",
        "    #calculating losses for each TPU core\n",
        "    per_replica_losses = strategy.run(train_step, args=(inp, targ, GRU_hidden1, GRU_hidden2, GRU_hidden3, teacher_forcing))\n",
        "    #back propogating to update losses\n",
        "    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n",
        "                          axis=None)\n",
        "\n",
        "  EPOCHS = 25\n",
        "  teacher_forcing = 1.0\n",
        "\n",
        "  #creating generator object to load data\n",
        "  dataset_test = load_data(train_data, BATCH_SIZE)\n",
        "  \n",
        "  initial = time.time()\n",
        "  for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    total_loss = 0\n",
        "\n",
        "    #initalizing hidden states\n",
        "    gru_hidden = gru.initialize_hidden_state()\n",
        "\n",
        "    for batch in range(batch_steps):\n",
        "      #loading next batch of data\n",
        "      inp = next(dataset_test)\n",
        "      \n",
        "      #training network on batch\n",
        "      batch_loss = distributed_train_step(inp, inp, gru_hidden, gru_hidden, gru_hidden, teacher_forcing)\n",
        "      total_loss += batch_loss\n",
        "      if batch % 100 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                    batch,\n",
        "                                                    batch_loss.numpy()))\n",
        "        \n",
        "    #saving weights every epoch\n",
        "    checkpoint.save(file_prefix=checkpoint_prefix, options=options)\n",
        "\n",
        "    print('Epoch {} Loss {:.4f} '.format(epoch + 1, total_loss / batch_steps))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.17.149.250:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.17.149.250:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "All devices:  [LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:7', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:6', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:5', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:4', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:3', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:0', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:1', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:2', device_type='TPU')]\n",
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 30.8058\n",
            "Epoch 1 Batch 100 Loss 11.0516\n",
            "Epoch 1 Batch 200 Loss 8.0252\n",
            "Epoch 1 Batch 300 Loss 6.9017\n",
            "Epoch 1 Batch 400 Loss 5.2825\n",
            "Epoch 1 Batch 500 Loss 6.2903\n",
            "Epoch 1 Batch 600 Loss 5.1282\n",
            "Epoch 1 Batch 700 Loss 4.4622\n",
            "Epoch 1 Batch 800 Loss 5.1664\n",
            "Epoch 1 Batch 900 Loss 4.9961\n",
            "Epoch 1 Batch 1000 Loss 4.6610\n",
            "Epoch 1 Batch 1100 Loss 4.1100\n",
            "Epoch 1 Batch 1200 Loss 3.7758\n",
            "Epoch 1 Batch 1300 Loss 4.3745\n",
            "Epoch 1 Batch 1400 Loss 4.4203\n",
            "Epoch 1 Batch 1500 Loss 4.7138\n",
            "Epoch 1 Loss 5.8173 \n",
            "Time taken for 1 epoch 1831.1882202625275 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 3.4837\n",
            "Epoch 2 Batch 100 Loss 3.9955\n",
            "Epoch 2 Batch 200 Loss 3.4166\n",
            "Epoch 2 Batch 300 Loss 3.8829\n",
            "Epoch 2 Batch 400 Loss 3.2336\n",
            "Epoch 2 Batch 500 Loss 4.8681\n",
            "Epoch 2 Batch 600 Loss 3.6180\n",
            "Epoch 2 Batch 700 Loss 3.4817\n",
            "Epoch 2 Batch 800 Loss 4.0911\n",
            "Epoch 2 Batch 900 Loss 3.8671\n",
            "Epoch 2 Batch 1000 Loss 3.7343\n",
            "Epoch 2 Batch 1100 Loss 3.4536\n",
            "Epoch 2 Batch 1200 Loss 3.1441\n",
            "Epoch 2 Batch 1300 Loss 3.6923\n",
            "Epoch 2 Batch 1400 Loss 3.6337\n",
            "Epoch 2 Batch 1500 Loss 3.9421\n",
            "Epoch 2 Loss 3.6352 \n",
            "Time taken for 1 epoch 1646.1427237987518 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 3.0031\n",
            "Epoch 3 Batch 100 Loss 3.4137\n",
            "Epoch 3 Batch 200 Loss 2.8845\n",
            "Epoch 3 Batch 300 Loss 3.3810\n",
            "Epoch 3 Batch 400 Loss 2.8256\n",
            "Epoch 3 Batch 500 Loss 4.4134\n",
            "Epoch 3 Batch 600 Loss 3.2331\n",
            "Epoch 3 Batch 700 Loss 3.2351\n",
            "Epoch 3 Batch 800 Loss 3.6969\n",
            "Epoch 3 Batch 900 Loss 3.4347\n",
            "Epoch 3 Batch 1000 Loss 3.4313\n",
            "Epoch 3 Batch 1100 Loss 3.1689\n",
            "Epoch 3 Batch 1200 Loss 2.9005\n",
            "Epoch 3 Batch 1300 Loss 3.4226\n",
            "Epoch 3 Batch 1400 Loss 3.3144\n",
            "Epoch 3 Batch 1500 Loss 3.6727\n",
            "Epoch 3 Loss 3.2672 \n",
            "Time taken for 1 epoch 1642.6199924945831 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 2.8458\n",
            "Epoch 4 Batch 100 Loss 3.1395\n",
            "Epoch 4 Batch 200 Loss 2.6791\n",
            "Epoch 4 Batch 300 Loss 3.1799\n",
            "Epoch 4 Batch 400 Loss 2.6426\n",
            "Epoch 4 Batch 500 Loss 4.1778\n",
            "Epoch 4 Batch 600 Loss 3.0348\n",
            "Epoch 4 Batch 700 Loss 3.1333\n",
            "Epoch 4 Batch 800 Loss 3.4655\n",
            "Epoch 4 Batch 900 Loss 3.2432\n",
            "Epoch 4 Batch 1000 Loss 3.2899\n",
            "Epoch 4 Batch 1100 Loss 2.9708\n",
            "Epoch 4 Batch 1200 Loss 2.7575\n",
            "Epoch 4 Batch 1300 Loss 3.1588\n",
            "Epoch 4 Batch 1400 Loss 3.1417\n",
            "Epoch 4 Batch 1500 Loss 3.5100\n",
            "Epoch 4 Loss 3.0783 \n",
            "Time taken for 1 epoch 1649.2592339515686 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 2.7601\n",
            "Epoch 5 Batch 100 Loss 2.9656\n",
            "Epoch 5 Batch 200 Loss 2.5601\n",
            "Epoch 5 Batch 300 Loss 3.0788\n",
            "Epoch 5 Batch 400 Loss 2.5287\n",
            "Epoch 5 Batch 500 Loss 3.9635\n",
            "Epoch 5 Batch 600 Loss 2.9131\n",
            "Epoch 5 Batch 700 Loss 3.0319\n",
            "Epoch 5 Batch 800 Loss 3.3430\n",
            "Epoch 5 Batch 900 Loss 3.1190\n",
            "Epoch 5 Batch 1000 Loss 3.2012\n",
            "Epoch 5 Batch 1100 Loss 2.8681\n",
            "Epoch 5 Batch 1200 Loss 2.6688\n",
            "Epoch 5 Batch 1300 Loss 3.0419\n",
            "Epoch 5 Batch 1400 Loss 3.0239\n",
            "Epoch 5 Batch 1500 Loss 3.3651\n",
            "Epoch 5 Loss 2.9572 \n",
            "Time taken for 1 epoch 1647.0686268806458 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 2.7213\n",
            "Epoch 6 Batch 100 Loss 2.8399\n",
            "Epoch 6 Batch 200 Loss 2.4830\n",
            "Epoch 6 Batch 300 Loss 2.9968\n",
            "Epoch 6 Batch 400 Loss 2.4516\n",
            "Epoch 6 Batch 500 Loss 3.8627\n",
            "Epoch 6 Batch 600 Loss 2.8164\n",
            "Epoch 6 Batch 700 Loss 2.9553\n",
            "Epoch 6 Batch 800 Loss 3.2259\n",
            "Epoch 6 Batch 900 Loss 3.0249\n",
            "Epoch 6 Batch 1000 Loss 3.1472\n",
            "Epoch 6 Batch 1100 Loss 2.7943\n",
            "Epoch 6 Batch 1200 Loss 2.6012\n",
            "Epoch 6 Batch 1300 Loss 2.9996\n",
            "Epoch 6 Batch 1400 Loss 2.9368\n",
            "Epoch 6 Batch 1500 Loss 3.2767\n",
            "Epoch 6 Loss 2.8688 \n",
            "Time taken for 1 epoch 1649.0602807998657 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 2.6741\n",
            "Epoch 7 Batch 100 Loss 2.7443\n",
            "Epoch 7 Batch 200 Loss 2.4176\n",
            "Epoch 7 Batch 300 Loss 2.9265\n",
            "Epoch 7 Batch 400 Loss 2.3926\n",
            "Epoch 7 Batch 500 Loss 3.7757\n",
            "Epoch 7 Batch 600 Loss 2.7558\n",
            "Epoch 7 Batch 700 Loss 2.9050\n",
            "Epoch 7 Batch 800 Loss 3.1218\n",
            "Epoch 7 Batch 900 Loss 2.9532\n",
            "Epoch 7 Batch 1000 Loss 3.1026\n",
            "Epoch 7 Batch 1100 Loss 2.7286\n",
            "Epoch 7 Batch 1200 Loss 2.5485\n",
            "Epoch 7 Batch 1300 Loss 2.9343\n",
            "Epoch 7 Batch 1400 Loss 2.8559\n",
            "Epoch 7 Batch 1500 Loss 3.1976\n",
            "Epoch 7 Loss 2.8013 \n",
            "Time taken for 1 epoch 1646.0411803722382 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 2.6312\n",
            "Epoch 8 Batch 100 Loss 2.6735\n",
            "Epoch 8 Batch 200 Loss 2.3726\n",
            "Epoch 8 Batch 300 Loss 2.8706\n",
            "Epoch 8 Batch 400 Loss 2.3450\n",
            "Epoch 8 Batch 500 Loss 3.7021\n",
            "Epoch 8 Batch 600 Loss 2.7054\n",
            "Epoch 8 Batch 700 Loss 2.8488\n",
            "Epoch 8 Batch 800 Loss 3.0406\n",
            "Epoch 8 Batch 900 Loss 2.8909\n",
            "Epoch 8 Batch 1000 Loss 3.0562\n",
            "Epoch 8 Batch 1100 Loss 2.6851\n",
            "Epoch 8 Batch 1200 Loss 2.5051\n",
            "Epoch 8 Batch 1300 Loss 2.8730\n",
            "Epoch 8 Batch 1400 Loss 2.7829\n",
            "Epoch 8 Batch 1500 Loss 3.1194\n",
            "Epoch 8 Loss 2.7444 \n",
            "Time taken for 1 epoch 1648.968798160553 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 2.6126\n",
            "Epoch 9 Batch 100 Loss 2.6143\n",
            "Epoch 9 Batch 200 Loss 2.3385\n",
            "Epoch 9 Batch 300 Loss 2.8303\n",
            "Epoch 9 Batch 400 Loss 2.3027\n",
            "Epoch 9 Batch 500 Loss 3.6407\n",
            "Epoch 9 Batch 600 Loss 2.6656\n",
            "Epoch 9 Batch 700 Loss 2.8001\n",
            "Epoch 9 Batch 800 Loss 2.9695\n",
            "Epoch 9 Batch 900 Loss 2.8388\n",
            "Epoch 9 Batch 1000 Loss 3.0177\n",
            "Epoch 9 Batch 1100 Loss 2.6459\n",
            "Epoch 9 Batch 1200 Loss 2.4707\n",
            "Epoch 9 Batch 1300 Loss 2.8115\n",
            "Epoch 9 Batch 1400 Loss 2.7241\n",
            "Epoch 9 Batch 1500 Loss 3.0464\n",
            "Epoch 9 Loss 2.6951 \n",
            "Time taken for 1 epoch 1670.5202724933624 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 2.5825\n",
            "Epoch 10 Batch 100 Loss 2.5614\n",
            "Epoch 10 Batch 200 Loss 2.3065\n",
            "Epoch 10 Batch 300 Loss 2.7943\n",
            "Epoch 10 Batch 400 Loss 2.2657\n",
            "Epoch 10 Batch 500 Loss 3.5906\n",
            "Epoch 10 Batch 600 Loss 2.6261\n",
            "Epoch 10 Batch 700 Loss 2.7535\n",
            "Epoch 10 Batch 800 Loss 2.8993\n",
            "Epoch 10 Batch 900 Loss 2.7957\n",
            "Epoch 10 Batch 1000 Loss 2.9858\n",
            "Epoch 10 Batch 1100 Loss 2.6090\n",
            "Epoch 10 Batch 1200 Loss 2.4431\n",
            "Epoch 10 Batch 1300 Loss 2.7580\n",
            "Epoch 10 Batch 1400 Loss 2.6674\n",
            "Epoch 10 Batch 1500 Loss 2.9950\n",
            "Epoch 10 Loss 2.6529 \n",
            "Time taken for 1 epoch 1673.479924440384 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 2.5675\n",
            "Epoch 11 Batch 100 Loss 2.5187\n",
            "Epoch 11 Batch 200 Loss 2.2804\n",
            "Epoch 11 Batch 300 Loss 2.7640\n",
            "Epoch 11 Batch 400 Loss 2.2354\n",
            "Epoch 11 Batch 500 Loss 3.5494\n",
            "Epoch 11 Batch 600 Loss 2.5945\n",
            "Epoch 11 Batch 700 Loss 2.7179\n",
            "Epoch 11 Batch 800 Loss 2.8351\n",
            "Epoch 11 Batch 900 Loss 2.7568\n",
            "Epoch 11 Batch 1000 Loss 2.9578\n",
            "Epoch 11 Batch 1100 Loss 2.5726\n",
            "Epoch 11 Batch 1200 Loss 2.4195\n",
            "Epoch 11 Batch 1300 Loss 2.7014\n",
            "Epoch 11 Batch 1400 Loss 2.6070\n",
            "Epoch 11 Batch 1500 Loss 2.9521\n",
            "Epoch 11 Loss 2.6155 \n",
            "Time taken for 1 epoch 1662.272391796112 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 2.5531\n",
            "Epoch 12 Batch 100 Loss 2.4792\n",
            "Epoch 12 Batch 200 Loss 2.2567\n",
            "Epoch 12 Batch 300 Loss 2.7372\n",
            "Epoch 12 Batch 400 Loss 2.2091\n",
            "Epoch 12 Batch 500 Loss 3.5139\n",
            "Epoch 12 Batch 600 Loss 2.5615\n",
            "Epoch 12 Batch 700 Loss 2.6835\n",
            "Epoch 12 Batch 800 Loss 2.7769\n",
            "Epoch 12 Batch 900 Loss 2.7220\n",
            "Epoch 12 Batch 1000 Loss 2.9316\n",
            "Epoch 12 Batch 1100 Loss 2.5380\n",
            "Epoch 12 Batch 1200 Loss 2.3997\n",
            "Epoch 12 Batch 1300 Loss 2.6398\n",
            "Epoch 12 Batch 1400 Loss 2.5515\n",
            "Epoch 12 Batch 1500 Loss 2.9133\n",
            "Epoch 12 Loss 2.5817 \n",
            "Time taken for 1 epoch 1663.0412907600403 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 2.5411\n",
            "Epoch 13 Batch 100 Loss 2.4445\n",
            "Epoch 13 Batch 200 Loss 2.2354\n",
            "Epoch 13 Batch 300 Loss 2.7127\n",
            "Epoch 13 Batch 400 Loss 2.1848\n",
            "Epoch 13 Batch 500 Loss 3.4788\n",
            "Epoch 13 Batch 600 Loss 2.5286\n",
            "Epoch 13 Batch 700 Loss 2.6527\n",
            "Epoch 13 Batch 800 Loss 2.7246\n",
            "Epoch 13 Batch 900 Loss 2.6916\n",
            "Epoch 13 Batch 1000 Loss 2.9047\n",
            "Epoch 13 Batch 1100 Loss 2.5065\n",
            "Epoch 13 Batch 1200 Loss 2.3807\n",
            "Epoch 13 Batch 1300 Loss 2.5946\n",
            "Epoch 13 Batch 1400 Loss 2.4965\n",
            "Epoch 13 Batch 1500 Loss 2.8837\n",
            "Epoch 13 Loss 2.5507 \n",
            "Time taken for 1 epoch 1656.9696254730225 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 2.5304\n",
            "Epoch 14 Batch 100 Loss 2.4147\n",
            "Epoch 14 Batch 200 Loss 2.2165\n",
            "Epoch 14 Batch 300 Loss 2.6914\n",
            "Epoch 14 Batch 400 Loss 2.1629\n",
            "Epoch 14 Batch 500 Loss 3.4509\n",
            "Epoch 14 Batch 600 Loss 2.5000\n",
            "Epoch 14 Batch 700 Loss 2.6261\n",
            "Epoch 14 Batch 800 Loss 2.6752\n",
            "Epoch 14 Batch 900 Loss 2.6634\n",
            "Epoch 14 Batch 1000 Loss 2.8782\n",
            "Epoch 14 Batch 1100 Loss 2.4788\n",
            "Epoch 14 Batch 1200 Loss 2.3627\n",
            "Epoch 14 Batch 1300 Loss 2.5682\n",
            "Epoch 14 Batch 1400 Loss 2.4482\n",
            "Epoch 14 Batch 1500 Loss 2.8689\n",
            "Epoch 14 Loss 2.5232 \n",
            "Time taken for 1 epoch 1660.413314819336 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 2.5243\n",
            "Epoch 15 Batch 100 Loss 2.3876\n",
            "Epoch 15 Batch 200 Loss 2.1993\n",
            "Epoch 15 Batch 300 Loss 2.6719\n",
            "Epoch 15 Batch 400 Loss 2.1430\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-7bc8c1037061>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m       \u001b[0;31m#loading next batch of data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m       \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m       \u001b[0;31m#training network on batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-1c274e5639d5>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(data, batch_size)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mpadded_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mencoded_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-0006229e3fcd>\u001b[0m in \u001b[0;36mone_hot_encoding\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;31m#one_hot_encode for batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mone_hot_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mencoded_smiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_smi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpad_smi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mencoded_smiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-0006229e3fcd>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;31m#one_hot_encode for batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mone_hot_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mencoded_smiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_smi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpad_smi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mencoded_smiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-4fac6cb2eb2b>\u001b[0m in \u001b[0;36mone_hot_encode\u001b[0;34m(self, tokenized_smiles)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mencoded_smiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msymbol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msymbol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_smiles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mencoded_smiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded_smiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_smiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_smiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_smiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mencoded_smiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1403\u001b[0m   \"\"\"\n\u001b[1;32m   1404\u001b[0m   return convert_to_tensor_v2(\n\u001b[0;32m-> 1405\u001b[0;31m       value, dtype=dtype, dtype_hint=dtype_hint, name=name)\n\u001b[0m\u001b[1;32m   1406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1413\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_hint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1540\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    263\u001b[0m   \"\"\"\n\u001b[1;32m    264\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 265\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    274\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m   \u001b[0;34m\"\"\"Implementation of eager constant.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}